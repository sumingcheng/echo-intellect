import os
import time
import logging
from typing import List, Dict, Optional, Tuple
from pathlib import Path

from app.models.data_models import Dataset, Collection, Data
from app.vectorstores.mongo_metadata import MongoMetadataStore
from app.vectorstores.milvus_store import MilvusVectorStore
from app.document_loaders.document_processor import DocumentProcessor
from app.embeddings.embedding_models import embedding_manager

logger = logging.getLogger(__name__)


class DataImportService:
    """æ•°æ®å¯¼å…¥æœåŠ¡ - æ”¯æŒæ‰¹é‡å¤„ç†ã€æ–­ç‚¹ç»­ä¼ å’Œæ¥å£è°ƒç”¨"""

    def __init__(self):
        self.mongo_store = MongoMetadataStore()
        self.milvus_store = MilvusVectorStore()
        self.processor = DocumentProcessor()
        self.batch_size = 10  # å‘é‡åŒ–æ‰¹æ¬¡å¤§å°

    async def initialize(self):
        """åˆå§‹åŒ–æ‰€æœ‰ç»„ä»¶"""
        try:
            logger.info("ğŸ”§ åˆå§‹åŒ–æ•°æ®å¯¼å…¥æœåŠ¡...")

            # åˆå§‹åŒ–å„ç»„ä»¶ï¼ˆæ ¹æ®å®é™…æƒ…å†µè°ƒæ•´async/syncï¼‰
            self.mongo_store.connect()

            # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
            if embedding_manager.embeddings is None:
                success = embedding_manager.initialize()
                if not success:
                    raise Exception("åµŒå…¥æ¨¡å‹åˆå§‹åŒ–å¤±è´¥")

            # è·å–æ­£ç¡®çš„å‘é‡ç»´åº¦
            dimension = embedding_manager.get_dimension()
            self.milvus_store.connect(dimension=dimension)

            logger.info("âœ… æ•°æ®å¯¼å…¥æœåŠ¡åˆå§‹åŒ–æˆåŠŸ")

            # æ£€æŸ¥å¹¶å¤„ç†æœªå®Œæˆçš„æ•°æ®
            await self._check_and_process_pending_data()

            return True

        except Exception as e:
            logger.error(f"âŒ åˆå§‹åŒ–å¤±è´¥: {e}")
            return False

    async def import_directory(
        self, data_dir: str = "./data", dataset_name: str = "æ–‡æ¡£çŸ¥è¯†åº“"
    ) -> Dict:
        """å¯¼å…¥æŒ‡å®šç›®å½•ä¸‹çš„æ‰€æœ‰txtæ–‡ä»¶"""
        result = {
            "success": False,
            "dataset_id": None,
            "files_processed": 0,
            "data_created": 0,
            "vectors_created": 0,
            "error": None,
        }

        try:
            # åˆ›å»ºæˆ–è·å–æ•°æ®é›†
            dataset = await self._get_or_create_dataset(dataset_name)
            result["dataset_id"] = dataset.id

            # æ‰«ææ–‡ä»¶
            txt_files = list(Path(data_dir).glob("*.txt"))
            if not txt_files:
                result["error"] = f"åœ¨ {data_dir} ç›®å½•ä¸‹æœªæ‰¾åˆ°txtæ–‡ä»¶"
                return result

            logger.info(f"ğŸ“‚ æ‰¾åˆ° {len(txt_files)} ä¸ªtxtæ–‡ä»¶")

            total_data = 0
            total_vectors = 0

            # å¤„ç†æ¯ä¸ªæ–‡ä»¶
            for file_path in txt_files:
                logger.info(f"ğŸ”„ å¤„ç†æ–‡ä»¶: {file_path.name}")

                file_result = await self._import_single_file(file_path, dataset.id)

                if file_result["success"]:
                    total_data += file_result["data_created"]
                    total_vectors += file_result["vectors_created"]
                    result["files_processed"] += 1
                else:
                    logger.error(f"âŒ æ–‡ä»¶å¤„ç†å¤±è´¥: {file_result['error']}")

            result["data_created"] = total_data
            result["vectors_created"] = total_vectors
            result["success"] = True

            logger.info("ğŸ‰ ç›®å½•å¯¼å…¥å®Œæˆï¼")

        except Exception as e:
            logger.error(f"âŒ ç›®å½•å¯¼å…¥å¤±è´¥: {e}")
            result["error"] = str(e)

        return result

    async def _import_single_file(self, file_path: Path, dataset_id: str) -> Dict:
        """å¯¼å…¥å•ä¸ªæ–‡ä»¶ - æ”¯æŒæ–­ç‚¹ç»­ä¼ """
        result = {
            "success": False,
            "data_created": 0,
            "vectors_created": 0,
            "error": None,
        }

        try:
            # åˆ›å»ºé›†åˆ
            collection_name = file_path.stem
            collection = await self._get_or_create_collection(
                collection_name, dataset_id
            )

            # æ£€æŸ¥æ˜¯å¦å·²æœ‰æœªå®Œæˆçš„æ•°æ®
            pending_data = await self._get_pending_data(collection.id)

            if pending_data:
                logger.info(f"ğŸ”„ å‘ç° {len(pending_data)} æ¡æœªå¤„ç†æ•°æ®ï¼Œç»§ç»­å¤„ç†...")
                await self._process_pending_data(pending_data)
                result["vectors_created"] = len(pending_data) * 2  # ä¸»å‘é‡+å­å‘é‡

            # è¯»å–æ–‡ä»¶å†…å®¹
            content = await self._read_file_with_encoding(file_path)
            if not content:
                result["error"] = f"æ— æ³•è¯»å–æ–‡ä»¶: {file_path}"
                return result

            # åˆ‡åˆ†æ–‡æ¡£
            chunks = await self._split_document(content)
            logger.info(f"ğŸ“ æ–‡æ¡£åˆ‡åˆ†ä¸º {len(chunks)} ä¸ªæ•°æ®å—")

            # ç¬¬ä¸€é˜¶æ®µï¼šæ‰¹é‡å­˜å‚¨åˆ°MongoDBï¼ˆæ ‡è®°ä¸ºæœªå¤„ç†ï¼‰
            data_list = await self._batch_store_chunks(chunks, collection.id)
            result["data_created"] = len(data_list)

            # ç¬¬äºŒé˜¶æ®µï¼šæ‰¹é‡å‘é‡åŒ–å¤„ç†
            vectors_created = await self._batch_vectorize_data(data_list)
            result["vectors_created"] = vectors_created

            result["success"] = True

        except Exception as e:
            logger.error(f"âŒ æ–‡ä»¶å¯¼å…¥å¤±è´¥: {e}")
            result["error"] = str(e)

        return result

    async def _read_file_with_encoding(self, file_path: Path) -> Optional[str]:
        """å°è¯•å¤šç§ç¼–ç è¯»å–æ–‡ä»¶"""
        encodings = ["utf-8", "gbk", "gb2312", "utf-16", "big5"]

        for encoding in encodings:
            try:
                with open(file_path, "r", encoding=encoding) as f:
                    content = f.read()
                    logger.info(f"âœ… ä½¿ç”¨ {encoding} ç¼–ç æˆåŠŸè¯»å–æ–‡ä»¶")
                    return content
            except UnicodeDecodeError:
                continue
            except Exception as e:
                logger.error(f"âŒ è¯»å–æ–‡ä»¶é”™è¯¯: {e}")
                return None

        logger.error("âŒ å°è¯•æ‰€æœ‰ç¼–ç éƒ½å¤±è´¥")
        return None

    async def _split_document(self, content: str) -> List[str]:
        """æ™ºèƒ½æ–‡æ¡£åˆ‡åˆ† - ä¼˜åŒ–ç‰ˆæœ¬ï¼Œç¡®ä¿1024å­—ç¬¦ä¸”ä¿æŒå¥å­å®Œæ•´æ€§"""
        target_chunk_size = 1024  # ç›®æ ‡å—å¤§å°
        min_chunk_size = 800      # æœ€å°å—å¤§å°
        max_chunk_size = 1200     # æœ€å¤§å—å¤§å°
        overlap_size = 100        # é‡å å¤§å°
        
        # ä¼˜åŒ–çš„åˆ†å‰²ä¼˜å…ˆçº§ï¼šæ®µè½ > ç« èŠ‚ > å¥å·+æ¢è¡Œ > æ„Ÿå¹å·+æ¢è¡Œ > é—®å·+æ¢è¡Œ > å¥å· > æ„Ÿå¹å· > é—®å· > æ¢è¡Œ
        split_patterns = [
            "\n\n\n",     # æ®µè½åˆ†éš”ï¼ˆ3ä¸ªæ¢è¡Œï¼‰
            "\n\n",       # æ®µè½åˆ†éš”ï¼ˆ2ä¸ªæ¢è¡Œï¼‰
            "ã€‚\n",       # å¥å·+æ¢è¡Œ
            "ï¼\n",       # æ„Ÿå¹å·+æ¢è¡Œ
            "ï¼Ÿ\n",       # é—®å·+æ¢è¡Œ
            "ï¼›\n",       # åˆ†å·+æ¢è¡Œ
            "ã€‚",         # å¥å·
            "ï¼",         # æ„Ÿå¹å·
            "ï¼Ÿ",         # é—®å·
            "ï¼›",         # åˆ†å·
            "ï¼š",         # å†’å·
            "\n",         # æ¢è¡Œ
        ]
        
        chunks = []
        start = 0
        
        logger.info(f"ğŸ“„ å¼€å§‹åˆ‡åˆ†æ–‡æ¡£ï¼Œæ€»é•¿åº¦: {len(content)} å­—ç¬¦")
        
        while start < len(content):
            # åˆå§‹ç›®æ ‡ç»“æŸä½ç½®
            target_end = start + target_chunk_size
            
            # å¦‚æœå‰©ä½™å†…å®¹å°äºæœ€å°å—å¤§å°ï¼Œç›´æ¥ä½œä¸ºæœ€åä¸€å—
            if len(content) - start <= min_chunk_size:
                remaining = content[start:].strip()
                if remaining:
                    chunks.append(remaining)
                    logger.debug(f"âœ… æœ€åä¸€å—: {len(remaining)} å­—ç¬¦")
                break
            
            # å¦‚æœå‰©ä½™å†…å®¹ä¸è¶³ç›®æ ‡å¤§å°ï¼Œç›´æ¥å–å®Œ
            if target_end >= len(content):
                remaining = content[start:].strip()
                if remaining:
                    chunks.append(remaining)
                    logger.debug(f"âœ… å‰©ä½™å†…å®¹å—: {len(remaining)} å­—ç¬¦")
                break
            
            # å¯»æ‰¾æœ€ä½³åˆ‡åˆ†ç‚¹
            best_split_pos = target_end
            best_pattern = None
            
            # åœ¨ç›®æ ‡ä½ç½®å‰åå¯»æ‰¾åˆé€‚çš„åˆ‡åˆ†ç‚¹
            search_start = max(start + min_chunk_size, target_end - 200)  # å‘å‰æœ€å¤šæœç´¢200å­—ç¬¦
            search_end = min(len(content), target_end + 200)              # å‘åæœ€å¤šæœç´¢200å­—ç¬¦
            
            for pattern in split_patterns:
                # å‘åæœç´¢ï¼ˆä¼˜å…ˆï¼‰
                forward_pos = content.find(pattern, target_end, search_end)
                if forward_pos != -1:
                    split_pos = forward_pos + len(pattern)
                    if split_pos - start <= max_chunk_size:  # ä¸è¶…è¿‡æœ€å¤§å¤§å°
                        best_split_pos = split_pos
                        best_pattern = pattern
                        break
                
                # å‘å‰æœç´¢
                backward_pos = content.rfind(pattern, search_start, target_end)
                if backward_pos != -1:
                    split_pos = backward_pos + len(pattern)
                    if split_pos - start >= min_chunk_size:  # ä¸å°äºæœ€å°å¤§å°
                        best_split_pos = split_pos
                        best_pattern = pattern
                        break
            
            # æå–å½“å‰å—
            chunk = content[start:best_split_pos].strip()
            if chunk:
                chunks.append(chunk)
                logger.debug(f"âœ… åˆ‡åˆ†å—: {len(chunk)} å­—ç¬¦ï¼Œåˆ†å‰²ç¬¦: {repr(best_pattern) if best_pattern else 'å¼ºåˆ¶åˆ‡åˆ†'}")
                
                # éªŒè¯å—å¤§å°
                if len(chunk) < min_chunk_size:
                    logger.warning(f"âš ï¸ å—è¿‡å°: {len(chunk)} < {min_chunk_size}")
                elif len(chunk) > max_chunk_size:
                    logger.warning(f"âš ï¸ å—è¿‡å¤§: {len(chunk)} > {max_chunk_size}")
            
            # è®¡ç®—ä¸‹ä¸€ä¸ªèµ·å§‹ä½ç½®ï¼ˆè€ƒè™‘é‡å ï¼‰
            next_start = max(best_split_pos - overlap_size, start + min_chunk_size)
            
            # é¿å…æ— é™å¾ªç¯
            if next_start <= start:
                next_start = start + min_chunk_size
            
            start = next_start
        
        # ç»Ÿè®¡ä¿¡æ¯
        if chunks:
            sizes = [len(chunk) for chunk in chunks]
            avg_size = sum(sizes) / len(sizes)
            logger.info(f"ğŸ“Š åˆ‡åˆ†å®Œæˆ: {len(chunks)} å—, å¹³å‡å¤§å°: {avg_size:.0f} å­—ç¬¦")
            logger.info(f"ğŸ“Š å¤§å°èŒƒå›´: {min(sizes)}-{max(sizes)} å­—ç¬¦")
            
            # æ£€æŸ¥è¿‡å°çš„å—
            small_chunks = [i for i, size in enumerate(sizes) if size < min_chunk_size]
            if small_chunks:
                logger.warning(f"âš ï¸ å‘ç° {len(small_chunks)} ä¸ªè¿‡å°çš„å—: {small_chunks}")
        
        return chunks

    async def _batch_store_chunks(
        self, chunks: List[str], collection_id: str
    ) -> List[Data]:
        """æ‰¹é‡å­˜å‚¨æ•°æ®å—åˆ°MongoDBï¼ˆæ ‡è®°ä¸ºæœªå¤„ç†ï¼‰"""
        data_list = []

        for i, chunk in enumerate(chunks):
            data_id = self._generate_data_id()

            data = Data(
                id=data_id,
                collection_id=collection_id,
                content=chunk,
                vector_ids=[],  # æš‚æ—¶ä¸ºç©º
                metadata={
                    "chunk_index": i,
                    "char_count": len(chunk),
                    "processed": False,  # æ ‡è®°ä¸ºæœªå¤„ç†
                },
            )

            # å­˜å‚¨åˆ°MongoDB
            try:
                self.mongo_store.create_data(data)
                data_list.append(data)

                if (i + 1) % 50 == 0:
                    logger.info(f"ğŸ“ å·²å­˜å‚¨ {i + 1}/{len(chunks)} ä¸ªæ•°æ®å—")

            except Exception as e:
                error_msg = str(e)
                if "E11000" in error_msg and "duplicate key" in error_msg:
                    # é‡å¤é”®é”™è¯¯ï¼Œç«‹å³åœæ­¢
                    logger.error(f"âŒ æ£€æµ‹åˆ°é‡å¤é”®é”™è¯¯ï¼Œç«‹å³åœæ­¢å¯¼å…¥: {error_msg}")
                    raise Exception(f"æ•°æ®å¯¼å…¥å¤±è´¥ï¼šæ£€æµ‹åˆ°é‡å¤æ•°æ®ï¼Œè¯·æ¸…ç†æ•°æ®åº“åé‡è¯•")
                else:
                    # å…¶ä»–é”™è¯¯ä¹Ÿåœæ­¢
                    logger.error(f"âŒ æ•°æ®å­˜å‚¨å¤±è´¥ï¼Œç«‹å³åœæ­¢: {error_msg}")
                    raise Exception(f"æ•°æ®å­˜å‚¨å¤±è´¥: {error_msg}")

        logger.info(f"âœ… å®Œæˆå­˜å‚¨ {len(data_list)} ä¸ªæ•°æ®å—åˆ°MongoDB")
        return data_list

    async def _batch_vectorize_data(self, data_list: List[Data]) -> int:
        """æ‰¹é‡å‘é‡åŒ–å¤„ç†"""
        total_vectors = 0
        total_batches = (
            len(data_list) + self.batch_size - 1
        ) // self.batch_size  # å‘ä¸Šå–æ•´

        # æŒ‰æ‰¹æ¬¡å¤„ç†
        for i in range(0, len(data_list), self.batch_size):
            batch_num = i // self.batch_size + 1
            batch = data_list[i : i + self.batch_size]

            try:
                # æ‰¹é‡ç”Ÿæˆå‘é‡
                batch_vectors = await self._generate_batch_vectors(batch)

                # æ‰¹é‡å­˜å‚¨åˆ°Milvus
                await self._store_batch_vectors(batch_vectors)

                # æ›´æ–°MongoDBçŠ¶æ€
                await self._update_batch_status(batch, batch_vectors)

                total_vectors += len(batch_vectors)

                # è®¡ç®—å¹¶æ˜¾ç¤ºè¿›åº¦ç™¾åˆ†æ¯”
                progress_percent = (batch_num / total_batches) * 100
                logger.info(
                    f"ğŸ”— æ‰¹æ¬¡ {batch_num}/{total_batches} å®Œæˆ | è¿›åº¦: {progress_percent:.1f}% | ç´¯è®¡å‘é‡: {total_vectors}"
                )

            except Exception as e:
                logger.error(
                    f"âŒ æ‰¹æ¬¡ {batch_num}/{total_batches} å¤„ç†å¤±è´¥ï¼Œç«‹å³åœæ­¢: {e}"
                )
                # ç«‹å³åœæ­¢ï¼Œä¸å†å¤„ç†åç»­æ‰¹æ¬¡
                raise Exception(f"æ‰¹é‡å‘é‡åŒ–å¤±è´¥: {e}")

        return total_vectors

    async def _generate_batch_vectors(self, data_batch: List[Data]) -> List[Dict]:
        """æ‰¹é‡ç”Ÿæˆå‘é‡"""
        vectors = []

        for data in data_batch:
            # ç”Ÿæˆä¸»å‘é‡
            main_vector_id = self._generate_vector_id()
            main_embedding = embedding_manager.embed_text(data.content)

            vectors.append(
                {
                    "id": main_vector_id,
                    "data_id": data.id,
                    "embedding": main_embedding,
                    "vector_type": "main",
                }
            )

            # ç”Ÿæˆå­å‘é‡ï¼ˆå‰512å­—ç¬¦ï¼‰
            if len(data.content) > 512:
                sub_content = data.content[:512]
                sub_vector_id = self._generate_vector_id()
                sub_embedding = embedding_manager.embed_text(sub_content)

                vectors.append(
                    {
                        "id": sub_vector_id,
                        "data_id": data.id,
                        "embedding": sub_embedding,
                        "vector_type": "sub",
                    }
                )

        return vectors

    async def _store_batch_vectors(self, vectors: List[Dict]):
        """æ‰¹é‡å­˜å‚¨å‘é‡åˆ°Milvus"""
        if not vectors:
            return

        from app.models.data_models import EmbeddingVector

        # è½¬æ¢ä¸ºEmbeddingVectorå¯¹è±¡
        embedding_vectors = []
        for vec in vectors:
            embedding_vector = EmbeddingVector(
                id=vec["id"],
                data_id=vec["data_id"],
                vector=vec["embedding"],
                model="bge-m3:latest",  # ä½¿ç”¨é»˜è®¤æ¨¡å‹åç§°
            )
            embedding_vectors.append(embedding_vector)

        # æ‰¹é‡æ’å…¥
        try:
            self.milvus_store.insert_vectors(embedding_vectors)
        except Exception as e:
            error_msg = str(e)
            logger.error(f"âŒ å‘é‡å­˜å‚¨å¤±è´¥ï¼Œç«‹å³åœæ­¢: {error_msg}")
            raise Exception(f"å‘é‡å­˜å‚¨å¤±è´¥: {error_msg}")

    async def _update_batch_status(self, data_batch: List[Data], vectors: List[Dict]):
        """æ‰¹é‡æ›´æ–°æ•°æ®çŠ¶æ€"""
        for data in data_batch:
            # è·å–è¯¥æ•°æ®çš„å‘é‡ID
            data_vectors = [v["id"] for v in vectors if v["data_id"] == data.id]

            # æ›´æ–°æ•°æ®
            data.vector_ids = data_vectors
            data.metadata["processed"] = True
            data.metadata["vector_count"] = len(data_vectors)

            self.mongo_store.update_data(data.id, data)

    async def _get_pending_data(self, collection_id: str) -> List[Data]:
        """è·å–æœªå¤„ç†çš„æ•°æ®"""
        return self.mongo_store.get_pending_data_by_collection(collection_id)

    async def _process_pending_data(self, pending_data: List[Data]):
        """å¤„ç†æœªå®Œæˆçš„æ•°æ®"""
        logger.info(f"ğŸ”„ ç»§ç»­å¤„ç† {len(pending_data)} æ¡æœªå®Œæˆæ•°æ®")
        await self._batch_vectorize_data(pending_data)

    async def _get_or_create_dataset(self, name: str) -> Dataset:
        """è·å–æˆ–åˆ›å»ºæ•°æ®é›†"""
        # æ£€æŸ¥æ˜¯å¦å­˜åœ¨
        existing = self.mongo_store.get_dataset_by_name(name)
        if existing:
            return existing

        # åˆ›å»ºæ–°æ•°æ®é›†
        dataset_id = self._generate_dataset_id()
        dataset = Dataset(
            id=dataset_id,
            name=name,
            description=f"è‡ªåŠ¨åˆ›å»ºçš„æ•°æ®é›†: {name}",
            collection_ids=[],
            metadata={"created_at": time.time()},
        )

        try:
            self.mongo_store.create_dataset(dataset)
            logger.info(f"âœ… åˆ›å»ºæ•°æ®é›†: {name} (ID: {dataset_id})")
            return dataset
        except Exception as e:
            error_msg = str(e)
            if "E11000" in error_msg and "duplicate key" in error_msg:
                logger.error(f"âŒ æ•°æ®é›†IDé‡å¤ï¼Œç«‹å³åœæ­¢: {error_msg}")
                raise Exception(f"æ•°æ®é›†åˆ›å»ºå¤±è´¥ï¼šIDé‡å¤ï¼Œè¯·é‡è¯•")
            else:
                logger.error(f"âŒ æ•°æ®é›†åˆ›å»ºå¤±è´¥: {error_msg}")
                raise Exception(f"æ•°æ®é›†åˆ›å»ºå¤±è´¥: {error_msg}")

    async def _get_or_create_collection(self, name: str, dataset_id: str) -> Collection:
        """è·å–æˆ–åˆ›å»ºé›†åˆ"""
        collection_id = self._generate_collection_id()
        collection = Collection(
            id=collection_id,
            dataset_id=dataset_id,
            name=name,
            description=f"ä»æ–‡ä»¶åˆ›å»º: {name}",
            data_ids=[],
            metadata={"created_at": time.time()},
        )

        try:
            self.mongo_store.create_collection(collection)
            logger.info(f"âœ… åˆ›å»ºé›†åˆ: {name} (ID: {collection_id})")
            return collection
        except Exception as e:
            error_msg = str(e)
            if "E11000" in error_msg and "duplicate key" in error_msg:
                logger.error(f"âŒ é›†åˆIDé‡å¤ï¼Œç«‹å³åœæ­¢: {error_msg}")
                raise Exception(f"é›†åˆåˆ›å»ºå¤±è´¥ï¼šIDé‡å¤ï¼Œè¯·é‡è¯•")
            else:
                logger.error(f"âŒ é›†åˆåˆ›å»ºå¤±è´¥: {error_msg}")
                raise Exception(f"é›†åˆåˆ›å»ºå¤±è´¥: {error_msg}")

    def _generate_dataset_id(self) -> str:
        """ç”Ÿæˆæ•°æ®é›†ID: 1 + æ—¶é—´æˆ³(6ä½) + è®¡æ•°å™¨(3ä½)"""
        timestamp = str(int(time.time()))[-6:]
        counter = "001"  # ç®€åŒ–ç‰ˆï¼Œå®é™…å¯ä»¥ç”¨Redisè®¡æ•°å™¨
        return f"1{timestamp}{counter}"

    def _generate_collection_id(self) -> str:
        """ç”Ÿæˆé›†åˆID: 2 + æ—¶é—´æˆ³(6ä½) + è®¡æ•°å™¨(3ä½)"""
        timestamp = str(int(time.time()))[-6:]
        counter = "001"
        return f"2{timestamp}{counter}"

    def _generate_data_id(self) -> str:
        """ç”Ÿæˆæ•°æ®ID: 3 + æ—¶é—´æˆ³(6ä½) + è®¡æ•°å™¨(5ä½)"""
        timestamp = str(int(time.time()))[-6:]
        counter = str(int(time.time() * 1000000))[-5:]  # å¾®ç§’ä½œä¸ºè®¡æ•°å™¨
        return f"3{timestamp}{counter}"

    def _generate_vector_id(self) -> str:
        """ç”Ÿæˆå‘é‡ID: 4 + æ—¶é—´æˆ³(6ä½) + è®¡æ•°å™¨(5ä½)"""
        timestamp = str(int(time.time()))[-6:]
        counter = str(int(time.time() * 1000000))[-5:]
        return f"4{timestamp}{counter}"

    async def get_import_status(self, dataset_id: str) -> Dict:
        """è·å–å¯¼å…¥çŠ¶æ€"""
        dataset = self.mongo_store.get_dataset(dataset_id)
        if not dataset:
            return {"error": "æ•°æ®é›†ä¸å­˜åœ¨"}

        # ç»Ÿè®¡ä¿¡æ¯
        collections = self.mongo_store.get_collections_by_dataset(dataset_id)
        total_data = 0
        processed_data = 0

        for collection in collections:
            data_list = self.mongo_store.get_data_by_collection(collection.id)
            total_data += len(data_list)
            processed_data += sum(
                1 for d in data_list if d.metadata.get("processed", False)
            )

        return {
            "dataset_id": dataset_id,
            "dataset_name": dataset.name,
            "total_collections": len(collections),
            "total_data": total_data,
            "processed_data": processed_data,
            "pending_data": total_data - processed_data,
            "progress": f"{processed_data}/{total_data}" if total_data > 0 else "0/0",
        }

    async def _check_and_process_pending_data(self):
        """æ£€æŸ¥å¹¶å¤„ç†å¯åŠ¨æ—¶æœªå®Œæˆçš„æ•°æ®"""
        try:
            logger.info("ğŸ” æ£€æŸ¥ç³»ç»Ÿä¸­æœªå¤„ç†çš„æ•°æ®...")

            # è·å–æœªå¤„ç†æ•°æ®è®¡æ•°
            pending_count = self.mongo_store.get_pending_data_count()

            if pending_count == 0:
                logger.info("âœ… æ²¡æœ‰æœªå¤„ç†çš„æ•°æ®ï¼Œç³»ç»ŸçŠ¶æ€æ­£å¸¸")
                return

            logger.info(f"ğŸ”„ å‘ç° {pending_count} æ¡æœªå¤„ç†æ•°æ®ï¼Œå¼€å§‹æ–­ç‚¹ç»­ä¼ å¤„ç†...")

            # è·å–æ‰€æœ‰æœªå¤„ç†æ•°æ®
            pending_data = self.mongo_store.get_all_pending_data()

            if not pending_data:
                logger.warning("âš ï¸ è®¡æ•°æ˜¾ç¤ºæœ‰æœªå¤„ç†æ•°æ®ï¼Œä½†æŸ¥è¯¢ç»“æœä¸ºç©º")
                return

            # æŒ‰é›†åˆåˆ†ç»„å¤„ç†
            collection_groups = {}
            for data in pending_data:
                collection_id = data.collection_id
                if collection_id not in collection_groups:
                    collection_groups[collection_id] = []
                collection_groups[collection_id].append(data)

            total_processed = 0
            total_vectors = 0

            # é€ä¸ªé›†åˆå¤„ç†
            for collection_id, data_list in collection_groups.items():
                logger.info(f"ğŸ“‚ å¤„ç†é›†åˆ {collection_id}: {len(data_list)} æ¡æ•°æ®")

                try:
                    # æ‰¹é‡å‘é‡åŒ–å¤„ç†
                    vectors_created = await self._batch_vectorize_data(data_list)
                    total_processed += len(data_list)
                    total_vectors += vectors_created

                    logger.info(
                        f"âœ… é›†åˆ {collection_id} å¤„ç†å®Œæˆ: {len(data_list)} æ•°æ®, {vectors_created} å‘é‡"
                    )

                except Exception as e:
                    logger.error(f"âŒ é›†åˆ {collection_id} å¤„ç†å¤±è´¥: {e}")
                    # è®°å½•é”™è¯¯ä½†ç»§ç»­å¤„ç†å…¶ä»–é›†åˆ
                    continue

            logger.info(f"ğŸ‰ æ–­ç‚¹ç»­ä¼ å¤„ç†å®Œæˆ!")
            logger.info(
                f"ğŸ“Š å¤„ç†ç»Ÿè®¡: {total_processed} æ¡æ•°æ®, {total_vectors} ä¸ªå‘é‡"
            )

        except Exception as e:
            logger.error(f"âŒ æ£€æŸ¥æœªå¤„ç†æ•°æ®å¤±è´¥: {e}")
            # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œé¿å…å½±å“ç³»ç»Ÿå¯åŠ¨
